# 20. Assembly Language Testing and Verification: Generating Certification Evidence for Safety-Critical Code

## Introduction: Testing as a Safety Case Pillar, Not an Afterthought

In safety-critical systems—where a single undetected assembly-level fault can lead to catastrophic failure—the act of testing is not merely about finding bugs. It is the primary mechanism for generating _certifiable evidence_ that the system behaves as required under all foreseeable conditions. Traditional software testing approaches, focused on functional coverage or code completion, are fundamentally inadequate for DO-178C Level A or IEC 62304 Class C systems.

Assembly language, by its nature, operates at the boundary of hardware and software. Its lack of abstraction makes it both powerful and perilous. A single misaligned stack pointer, an unverified register clobber, or an unchecked branch target can bypass high-level safeguards entirely. Therefore, **testing assembly code must be structured, traceable, and explicitly designed to produce verifiable evidence** that directly links test outcomes to safety requirements.

This tutorial transforms testing from a routine activity into a formal verification discipline. We will establish patterns that ensure every test case generates certification-grade evidence, every failure mode is documented with a safety rationale, and every line of assembly is proven safe—not just correct.

> **Safety Philosophy**: _If you cannot trace a test case back to a requirement and demonstrate its impact on a safety property, it is not a test—it is noise._

---

## Why Traditional Assembly Testing Fails in Safety-Critical Contexts

| Problem (Traditional Approach)                  | Consequence in Safety-Critical Systems                                |
| ----------------------------------------------- | --------------------------------------------------------------------- |
| Tests written after code completion             | Gaps between implementation and verification; no traceability         |
| Focus on “code works” vs. “safety preserved”    | Misses critical error paths and state corruption modes                |
| No standardized verification tagging            | Cannot automate evidence extraction for certification audits          |
| Manual test case generation                     | Incomplete coverage, especially for edge cases and processor variants |
| Lack of error path quantification               | Cannot prove 100% error path coverage required for Level A            |
| Tests embedded in scripts without documentation | Evidence not auditable, not traceable to requirements                 |
| No tool qualification for test harnesses        | Evidence generated by unqualified tools is invalid per DO-178C        |
| Ignoring processor-specific behavior            | Test passes on dev machine, fails on target hardware                  |

### Case Study: Avionics Flight Control Failure Due to Unverified Test Coverage

A flight control unit experienced intermittent pitch instability during high-G maneuvers. Post-failure analysis revealed that the assembly-based altitude hold algorithm had been tested only under nominal conditions. The critical error path—where a corrupted sensor value caused an overflow during altitude adjustment—was never exercised because the test suite lacked:

- A test case simulating invalid input data
- Verification of the error code propagation path
- Cross-processor testing on the actual flight hardware variant
- Tool-qualified coverage analyzer to prove 100% MC/DC on error branches

The certification body rejected the evidence package because **no test demonstrated coverage of the `ERROR_CALCULATION` path**—a requirement explicitly mandated by the safety analysis (SAR-107). Furthermore, the test harness used Python scripts that were not tool-qualified, rendering all test logs invalid.

> **Lesson**: In safety-critical systems, _untested error paths are treated as unmitigated hazards_. Certification bodies require proof that every possible failure mode has been intentionally tested — and that the tools used to generate that proof are themselves qualified.

---

## Assembly Testing Philosophy for Safety-Critical Development

Assembly testing must adhere to three non-negotiable principles:

1. **Traceability**: Every test case must be linked to one or more requirements (`REQ-XXX`) and verification objectives (`VC-XXX`).
2. **Evidence Generation**: Every test execution must produce machine-readable, auditable output (logs, coverage reports, assertion logs) tied to specific verification tags.
3. **Safety Preservation**: Tests must validate not just functionality, but the preservation of safety properties (e.g., register state, stack integrity, error state propagation, timing bounds).

> **Core Tenet**: _A test that does not generate certification evidence is not a valid test in a safety-critical context._

### Expanded Principle: The Four Pillars of Safety-Critical Assembly Testing

| Pillar                    | Description                                                                | Certification Impact                           |
| ------------------------- | -------------------------------------------------------------------------- | ---------------------------------------------- |
| **Verifiability**         | Every test must be repeatable, automated, and produce deterministic output | DO-178C Section 6.4.2.1 — Verification Process |
| **Traceability**          | Every test maps to a requirement and a verification objective              | DO-178C Section 5.2 — Traceability             |
| **Coverage Completeness** | 100% statement, branch, and MC/DC coverage for Level A                     | DO-178C Section 6.4.4.2 — Structural Coverage  |
| **Tool Qualification**    | All test tools (harnesses, coverage analyzers, loggers) must be qualified  | DO-178C Section 12.2 — Tool Qualification      |

---

## Fundamental Patterns for Verifiable Assembly Testing

We define five core testing patterns, each with associated verification annotations, evidence templates, and tool qualification notes.

### Pattern 1: Structured Unit Testing with Verification Tags and Coverage Hooks

Unit tests for assembly functions must verify individual behavior _in isolation_, including side effects, register usage, error states, and worst-case timing.

#### Safe Pattern: Verified Unit Test Template with Coverage Instrumentation

```x86asm
;# Summary: Unit test for calculate_altitude function with coverage hooks
;# Requirement: REQ-UNIT-001
;# Verification: VC-UNIT-001
;# Test: TEST-UNIT-001
;#
;# Test Objective: Verify that calculate_altitude correctly handles valid input and returns expected output with zero error code.
;#
;# Safety Rationale: Functional correctness of the calculation path is foundational to altitude safety.
;# Failure Mode: Incorrect calculation leads to erroneous control commands.
;# Interface Safety: Ensures return values meet contract defined in REQ-EXP-007.
;# Coverage Target: 100% statement coverage of main path
;#
;# Tool: GNU Assembler 2.38 + Custom Test Harness (Qualified per DO-178C Tool Qualification Guide, TQ-ID: ASM-HARNESS-001)
;# Environment: x64 Linux, Intel Core i7-1185G7
;# Coverage Method: Instrumented via debug symbols + custom breakpoint script

.section .text
.global _test_calculate_altitude_valid_input

_test_calculate_altitude_valid_input:
    ;# check: REQ-UNIT-002
    ;# check: VC-UNIT-002
    ; Setup: Load known good values
    mov $10000, %rdi        ; current_altitude = 10000 ft
    mov $5000, %rsi         ; altitude_adjustment = +5000 ft
    xor %rdx, %rdx          ; clear error code register

    ; Call target function
    call calculate_altitude

    ;# check: REQ-UNIT-003
    ;# check: VC-UNIT-003
    ; Verify return value: rax = 15000
    cmp $15000, %rax
    jne test_fail_rax

    ;# check: REQ-UNIT-004
    ;# check: VC-UNIT-004
    ; Verify error code: rdx = 0
    cmp $0, %rdx
    jne test_fail_rdx

    ;# check: REQ-UNIT-005
    ;# check: VC-UNIT-005
    ; Verify callee-saved registers preserved (RBX, R12-R15)
    push %rbx
    push %r12
    push %r13
    push %r14
    push %r15
    pop %r15
    pop %r14
    pop %r13
    pop %r12
    pop %rbx
    ; If we reach here, registers were preserved

    ;# check: REQ-UNIT-006
    ;# check: VC-UNIT-006
    ; Verify stack pointer restored
    mov %rsp, %rax
    and $15, %rax           ; Check 16-byte alignment
    test %rax, %rax
    jnz test_fail_stack

    jmp test_pass

test_fail_rax:
    mov $1, %rax            ; Indicate test failure via rax
    ret

test_fail_rdx:
    mov $2, %rax            ; Indicate test failure via rax
    ret

test_fail_stack:
    mov $3, %rax            ; Stack misalignment failure
    ret

test_pass:
    mov $0, %rax            ; Indicate test success via rax
    ret
```

> **Evidence Generation Note**: This test outputs `%rax = 0` on success, `1` if `rax` fails, `2` if `rdx` fails, `3` if stack fails. A qualified test harness reads this value and logs:  
> `TEST-UNIT-001 PASSED: REQ-UNIT-001 verified. VC-UNIT-001 satisfied. Coverage: 100% of main path.`  
> **Coverage Command**: `objdump -d calculate_altitude.o | grep -A 10 "<calculate_altitude>" > coverage_main_path.log`

#### Risky Pattern to Avoid

```x86asm
; DON’T DO THIS — NO VERIFICATION TAGS, NO TRACEABILITY, NO COVERAGE HOOKS
test_calc:
    mov $10000, %rdi
    mov $5000, %rsi
    call calculate_altitude
    ; ... assume it worked
```

→ **Certification Risk**: Zero evidence generated. Cannot prove what was tested or why. Unqualified test execution.

---

### Pattern 2: Error Path Coverage Testing with Formal Verification Matrix and MC/DC Analysis

DO-178C requires **100% error path coverage** for Level A systems. This means every `jg`, `jnz`, `je`, etc., leading to an error handler must be individually tested — and for MC/DC, every condition in a compound branch must be shown to independently affect the outcome.

#### Safe Pattern: Error Path Test Matrix with MC/DC Evidence

```x86asm
;# Summary: Error path coverage matrix for calculate_altitude with MC/DC
;# Requirement: REQ-ERR-001
;# Verification: VC-ERR-001
;# Test: TEST-ERR-001
;#
;# Objective: Achieve 100% error path coverage for all error exit points, including MC/DC for compound conditions.
;# Safety Rationale: Unexercised error paths represent latent hazards.
;# Failure Mode: System may fail silently under fault conditions.
;# MC/DC Target: All conditions in compound branches independently affect outcome.
;#
;# Tool: GNU Assembler 2.38 + Custom Coverage Analyzer (Qualified, TQ-ID: COV-ASM-002)
;# Coverage Method: Instruction-level coverage via debug symbols + breakpoint logging + MC/DC condition tracking
;# Targets: 5 error paths (stack alignment, parameter, altitude limit, calc overflow, null ptr)

.section .text
.global _test_error_paths

_test_error_paths:
    ;# check: REQ-ERR-002
    ;# check: VC-ERR-002
    ; Test 1: Stack Alignment Error
    ; Force misalignment by adjusting stack before call
    sub $1, %rsp            ; Break 16-byte alignment
    mov $10000, %rdi
    mov $5000, %rsi
    call calculate_altitude
    cmp $3, %rdx            ; Expect ERROR_STACK (3)
    jne test_err_1_fail
    inc %r8                 ; Counter for passed error paths
test_err_1_pass:
    ;# check: REQ-ERR-003
    ;# check: VC-ERR-003
    ; Test 2: Parameter Error (current_altitude > MAX_ALTITUDE)
    mov $50001, %rdi        ; Exceeds MAX_ALTITUDE (50000)
    mov $0, %rsi
    call calculate_altitude
    cmp $1, %rdx            ; Expect ERROR_PARAMETER (1)
    jne test_err_2_fail
    inc %r8
test_err_2_pass:
    ;# check: REQ-ERR-004
    ;# check: VC-ERR-004
    ; Test 3: Calculation Overflow (result > MAX_ALTITUDE)
    mov $49999, %rdi        ; Near limit
    mov $2, %rsi            ; Causes overflow
    call calculate_altitude
    cmp $2, %rdx            ; Expect ERROR_CALCULATION (2)
    jne test_err_3_fail
    inc %r8
test_err_3_pass:
    ;# check: REQ-ERR-005
    ;# check: VC-ERR-005
    ; Test 4: Null Pointer (via wrapper)
    xor %rdi, %rdi          ; NULL pointer
    call calculate_altitude
    cmp $1, %rdx            ; Wrapper should set ERROR_PARAMETER on null
    jne test_err_4_fail
    inc %r8
test_err_4_pass:
    ;# check: REQ-ERR-006
    ;# check: VC-ERR-006
    ; Test 5: Invalid Input (negative adjustment causing underflow)
    mov $0, %rdi            ; Altitude = 0
    mov $-100000, %rsi      ; Massive negative adjustment
    call calculate_altitude
    cmp $2, %rdx            ; Should trigger overflow check due to wraparound
    jne test_err_5_fail
    inc %r8
test_err_5_pass:
    ;# check: REQ-ERR-007
    ;# check: VC-ERR-007
    ; Final verification: Did we hit all 5 error paths?
    cmp $5, %r8             ; Must have incremented 5 times
    jne test_all_failed
    mov $0, %rax            ; All error paths passed
    ret

test_err_1_fail:
    mov $10, %rax
    ret
test_err_2_fail:
    mov $11, %rax
    ret
test_err_3_fail:
    mov $12, %rax
    ret
test_err_4_fail:
    mov $13, %rax
    ret
test_err_5_fail:
    mov $14, %rax
    ret
test_all_failed:
    mov $15, %rax
    ret
```

> **Evidence Output**:  
> The test returns `%rax = 0` only if all 5 error paths were triggered and validated.  
> **Certification Evidence Generated**:
>
> - Log file: `TEST-ERR-001.log` → `Error Paths Covered: 5/5 (100%)`
> - MC/DC Report: `mcdc_calculate_altitude.csv` → `Conditions: 3, Covered: 3, MC/DC: 100%`
> - Traceability Matrix: `REQ-ERR-001 → VC-ERR-001 → TEST-ERR-001 → [5 test cases]`
> - Coverage report: `Instruction coverage of all error handlers: 100%`

#### MC/DC Evidence Template (CSV)

```csv
Function,Branch,Condition,Test_Case,Outcome,Independent_Effect
calculate_altitude,"cmp $MAX_ALTITUDE, %rdi; jg parameter_error",rdi > MAX_ALTITUDE,TEST-ERR-002,TRUE,Yes
calculate_altitude,"cmp $MAX_ALTITUDE, %rdi; jg parameter_error",rdi <= MAX_ALTITUDE,TEST-UNIT-001,FALSE,Yes
calculate_altitude,"add %rsi, %rdi; jo calculation_error",overflow_occurred,TEST-ERR-003,TRUE,Yes
calculate_altitude,"add %rsi, %rdi; jo calculation_error",no_overflow,TEST-UNIT-001,FALSE,Yes
calculate_altitude,"test %rdi, %rdi; jz parameter_error",rdi == 0,TEST-ERR-004,TRUE,Yes
calculate_altitude,"test %rdi, %rdi; jz parameter_error",rdi != 0,TEST-UNIT-001,FALSE,Yes
```

> **Tool Command for MC/DC Extraction**:  
> `python3 mcdc_extractor.py --elf calculate_altitude.o --testlog TEST-ERR-001.log --output mcdc_calculate_altitude.csv`

---

### Pattern 3: State Preservation & Register Integrity Testing with Memory Snapshot Comparison

Assembly functions must preserve caller-saved and callee-saved registers. A failure here corrupts the entire calling context. We extend Pattern 1 by adding memory snapshot comparison for stack and global state.

#### Safe Pattern: Register and Memory State Verification Test

```x86asm
;# Summary: Register and memory state preservation test for calculate_altitude
;# Requirement: REQ-REG-001
;# Verification: VC-REG-001
;# Test: TEST-REG-001
;#
;# Objective: Verify that calculate_altitude preserves all callee-saved registers AND does not corrupt adjacent stack/global memory.
;# Safety Rationale: Corrupted registers or memory cause cascading failures in higher layers.
;# Failure Mode: C/C++ caller's variables become garbage after assembly call.
;# Interface Safety: Ensures ABI compliance beyond mere function signature.
;# Memory Safety: Verify no unintended writes to stack/global regions.
;#
;# Tool: GNU Assembler 2.38 + GDB Script (Qualified, TQ-ID: GDB-ASM-003)
;# Environment: x64 Linux, Intel Core i7-1185G7
;# Coverage Method: Pre/post memory snapshot comparison via GDB

.section .data
saved_regs:
    .quad 0x1111111111111111  ; RBX
    .quad 0x2222222222222222  ; R12
    .quad 0x3333333333333333  ; R13
    .quad 0x4444444444444444  ; R14
    .quad 0x5555555555555555  ; R15

; Canary values to detect memory corruption
stack_canary_before:
    .quad 0xDEADBEEFDEADBEEF
    .quad 0xDEADBEEFDEADBEEF
    .quad 0xDEADBEEFDEADBEEF
stack_canary_after:
    .quad 0xDEADBEEFDEADBEEF
    .quad 0xDEADBEEFDEADBEEF
    .quad 0xDEADBEEFDEADBEEF

.section .text
.global _test_register_and_memory_preservation

_test_register_and_memory_preservation:
    ;# check: REQ-REG-002
    ;# check: VC-REG-002
    ; Load known values into callee-saved registers
    mov $0x1111111111111111, %rbx
    mov $0x2222222222222222, %r12
    mov $0x3333333333333333, %r13
    mov $0x4444444444444444, %r14
    mov $0x5555555555555555, %r15

    ;# check: REQ-REG-003
    ;# check: VC-REG-003
    ; Save original values to memory for later comparison
    mov %rbx, saved_regs
    mov %r12, saved_regs+8
    mov %r13, saved_regs+16
    mov %r14, saved_regs+24
    mov %r15, saved_regs+32

    ;# check: REQ-REG-004
    ;# check: VC-REG-004
    ; Verify canary values are intact BEFORE call
    mov stack_canary_before, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_before
    mov stack_canary_before+8, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_before
    mov stack_canary_before+16, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_before

    ;# check: REQ-REG-005
    ;# check: VC-REG-005
    ; Call target function
    mov $10000, %rdi
    mov $5000, %rsi
    call calculate_altitude

    ;# check: REQ-REG-006
    ;# check: VC-REG-006
    ; Compare restored register values against saved ones
    cmp saved_regs, %rbx
    jne reg_fail_rbx
    cmp saved_regs+8, %r12
    jne reg_fail_r12
    cmp saved_regs+16, %r13
    jne reg_fail_r13
    cmp saved_regs+24, %r14
    jne reg_fail_r14
    cmp saved_regs+32, %r15
    jne reg_fail_r15

    ;# check: REQ-REG-007
    ;# check: VC-REG-007
    ; Verify canary values are intact AFTER call
    mov stack_canary_after, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_after
    mov stack_canary_after+8, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_after
    mov stack_canary_after+16, %rax
    cmp $0xDEADBEEFDEADBEEF, %rax
    jne canary_corrupted_after

    ;# check: REQ-REG-008
    ;# check: VC-REG-008
    ; All registers and memory preserved
    mov $0, %rax
    ret

canary_corrupted_before:
    mov $6, %rax
    ret
canary_corrupted_after:
    mov $7, %rax
    ret
reg_fail_rbx:
    mov $1, %rax
    ret
reg_fail_r12:
    mov $2, %rax
    ret
reg_fail_r13:
    mov $3, %rax
    ret
reg_fail_r14:
    mov $4, %rax
    ret
reg_fail_r15:
    mov $5, %rax
    ret
```

> **Evidence Output**:
>
> - Pass: `%rax = 0` → All registers and memory preserved.
> - Fail: `%rax = N` → Register N or canary corrupted.
> - Automated log: `TEST-REG-001: RBX=0x111... ✓, R12=0x222... ✓, ..., CanaryAfter=0xDEAD... ✓ → PASS`
> - Certification artifact: `MemoryIntegrityTest_Report.md` with hex dumps pre/post-call and GDB memory diff.

---

### Pattern 4: Timing and Worst-Case Execution Time (WCET) Verification

For real-time safety-critical systems, timing is a safety property. We must prove that assembly functions meet their timing deadlines under worst-case conditions.

#### Safe Pattern: WCET Measurement Test with Hardware Timer

```x86asm
;# Summary: WCET measurement for calculate_altitude using RDTSC
;# Requirement: REQ-TIME-001
;# Verification: VC-TIME-001
;# Test: TEST-TIME-001
;#
;# Objective: Measure worst-case execution time of calculate_altitude under stress conditions.
;# Safety Rationale: Missing timing deadline can cause system failure.
;# Failure Mode: Function exceeds WCET bound, causing missed control cycle.
;# Timing Safety: Must prove WCET < 200 cycles for 10kHz control loop.
;#
;# Tool: GNU Assembler 2.38 + Custom Timer Script (Qualified, TQ-ID: TIMER-ASM-004)
;# Environment: x64 Linux, Intel Core i7-1185G7 (disable turbo, set governor to performance)
;# Method: RDTSC before/after, 1000 iterations, record maximum

.section .data
wcet_max: .quad 0
iteration_count: .quad 1000

.section .text
.global _test_wcet_calculate_altitude

_test_wcet_calculate_altitude:
    mov iteration_count, %rcx
    xor %r8, %r8            ; r8 = max cycles

test_loop:
    ;# check: REQ-TIME-002
    ;# check: VC-TIME-002
    ; Read timestamp before
    rdtsc
    shl $32, %rdx
    or %rdx, %rax
    mov %rax, %r9           ; save start time

    ; Call function with worst-case inputs (triggers error path)
    mov $50001, %rdi        ; Forces parameter error
    mov $0, %rsi
    call calculate_altitude

    ;# check: REQ-TIME-003
    ;# check: VC-TIME-003
    ; Read timestamp after
    rdtsc
    shl $32, %rdx
    or %rdx, %rax
    sub %r9, %rax           ; rax = cycles elapsed

    ; Update max if needed
    cmp %r8, %rax
    cmovg %rax, %r8         ; r8 = max(r8, rax)

    loop test_loop

    ;# check: REQ-TIME-004
    ;# check: VC-TIME-004
    ; Verify WCET < 200 cycles
    cmp $200, %r8
    jg wcet_exceeded

    ; Store result
    mov %r8, wcet_max
    mov $0, %rax            ; PASS
    ret

wcet_exceeded:
    mov $1, %rax            ; FAIL
    ret
```

> **Evidence Output**:
>
> - Pass: `%rax = 0`, `wcet_max = 147` → WCET within bound.
> - Fail: `%rax = 1` → WCET exceeded.
> - Log: `TEST-TIME-001: WCET=147 cycles (bound=200) → PASS`
> - Certification artifact: `WCET_Report.pdf` with histogram of 1000 runs, min/avg/max, and environmental conditions.

---

### Pattern 5: Mutation Testing for Assembly — Injecting Faults to Validate Test Suite

To prove your test suite is robust, you must show it can detect injected faults. Mutation testing modifies the assembly code (e.g., flip a branch condition) and verifies the test suite fails.

#### Safe Pattern: Mutation Test Script (Python) for Branch Inversion

```python
#!/usr/bin/env python3
;# Summary: Mutation testing script for calculate_altitude
;# Requirement: REQ-MUT-001
;# Verification: VC-MUT-001
;# Test: TEST-MUT-001
;#
;# Objective: Validate that test suite detects injected faults (branch inversion).
;# Safety Rationale: Test suite must be sensitive to safety-critical faults.
;# Failure Mode: Test suite passes even when code is corrupted.
;# Mutation Safety: Prove test suite kills mutants.
;#
;# Tool: Python 3.10 + Custom Mutator (Qualified, TQ-ID: MUTATOR-PY-005)
;# Method: Parse assembly, invert branch conditions, reassemble, run test suite

import os
import subprocess

def mutate_branch_inversion(asm_file, output_file, line_number):
    with open(asm_file, 'r') as f:
        lines = f.readlines()

    # Find branch instruction at line_number (e.g., "jg label" -> "jle label")
    target_line = lines[line_number - 1]  # 1-indexed
    mutated_line = target_line
    branch_map = {
        'jg ': 'jle ',
        'jl ': 'jge ',
        'je ': 'jne ',
        'jne ': 'je ',
        'jge ': 'jl ',
        'jle ': 'jg ',
        'ja ': 'jbe ',
        'jb ': 'jae ',
    }
    for orig, new in branch_map.items():
        if orig in target_line:
            mutated_line = target_line.replace(orig, new)
            break

    lines[line_number - 1] = mutated_line
    with open(output_file, 'w') as f:
        f.writelines(lines)

def assemble_and_test(mutated_asm, test_harness):
    # Assemble mutated code
    subprocess.run(['gcc', '-c', mutated_asm, '-o', 'mutated.o'], check=True)
    # Link with test harness
    subprocess.run(['gcc', 'mutated.o', test_harness, '-o', 'mutated_test'], check=True)
    # Run test
    result = subprocess.run(['./mutated_test'], capture_output=True)
    return result.returncode != 0  # True if test FAILS (mutant killed)

# Main: Mutate each branch and verify test suite fails
branches_to_mutate = [45, 58, 72]  # Line numbers of 'jg', 'jne', etc. in calculate_altitude.asm
for line_num in branches_to_mutate:
    mutated_file = f"calculate_altitude_mutated_line{line_num}.asm"
    mutate_branch_inversion("calculate_altitude.asm", mutated_file, line_num)
    if not assemble_and_test(mutated_file, "test_harness.c"):
        print(f"ERROR: Mutant at line {line_num} NOT KILLED — test suite insufficient!")
        exit(1)
    else:
        print(f"PASS: Mutant at line {line_num} killed by test suite.")

print("All mutants killed. Test suite is robust.")
```

> **Evidence Output**:
>
> - Console log: `PASS: Mutant at line 45 killed...`
> - Certification artifact: `MutationTest_Report.md` listing all mutants, locations, and kill status.
> - Requirement: `REQ-MUT-001: Test suite shall detect 100% of injected branch faults.`

---

## Certification Evidence Templates — Expanded

Every test must generate standardized, auditable evidence. Below are expanded templates for inclusion in your certification package.

### Template 1: Test Traceability Matrix (CSV Format) — Extended

```csv
Requirement ID,Verification ID,Test ID,Description,Status,Coverage,Notes,Tool_Qual_ID
REQ-UNIT-001,VC-UNIT-001,TEST-UNIT-001,"Valid input produces correct output",PASSED,100%,"Main path only",ASM-HARNESS-001
REQ-ERR-001,VC-ERR-001,TEST-ERR-001,"All 5 error paths executed",PASSED,100%,"Covered: stack, param, calc, null, underflow",COV-ASM-002
REQ-REG-001,VC-REG-001,TEST-REG-001,"Callee-saved registers preserved",PASSED,100%,"All 5 regs + memory canaries",GDB-ASM-003
REQ-TIME-001,VC-TIME-001,TEST-TIME-001,"WCET < 200 cycles",PASSED,100%,"1000 iterations, max=147",TIMER-ASM-004
REQ-MUT-001,VC-MUT-001,TEST-MUT-001,"Mutation testing kills 100% of branch mutants",PASSED,100%,"3 mutants killed",MUTATOR-PY-005
REQ-IF-TRACE-001,VC-IF-TRACE-001,TEST-IF-TRACE-001,"Interface traceability complete",PASSED,100%,"All #check tags mapped",ASM-HARNESS-001
```

### Template 2: Verification Report Snippet — Full Multi-Page Example

## VERIFICATION REPORT: TEST-ERR-001 — ERROR PATH & MC/DC COVERAGE

| Field         | Value                                                             |
| ------------- | ----------------------------------------------------------------- |
| Component     | calculate_altitude                                                |
| Requirement   | REQ-ERR-001                                                       |
| Verification  | VC-ERR-001                                                        |
| Test Executed | TEST-ERR-001                                                      |
| Tool          | GNU Assembler 2.38 + Custom Coverage Script (Qual ID: TQ-ASM-007) |
| Environment   | Ubuntu 22.04, Intel Core i7-1185G7, x64                           |
| Date          | 2025-04-05                                                        |
| Tester        | Jane Doe, Lead Verification Engineer                              |

### 1. Test Objective

Achieve 100% error path coverage and Modified Condition/Decision Coverage (MC/DC) for all branches in `calculate_altitude`.

### 2. Test Methodology

- Executed 5 targeted test cases, each triggering a unique error path.
- Instrumented code with debug symbols.
- Used qualified coverage analyzer to log branch execution.
- Generated MC/DC report by varying each condition independently.

### 3. Results

- **Total Error Paths Identified**: 5
- **Paths Tested**: 5
- **Coverage**: 100%
- **MC/DC Coverage**: 100% (3 conditions, all shown to independently affect outcome)
- **Pass/Fail**: PASSED

### 4. Evidence Artifacts

- `logs/test_err_001.log`: Raw execution log with register states and branch traces.
- `coverage/error_paths.csv`: Machine-readable instruction coverage map.
- `coverage/mcdc_calculate_altitude.csv`: MC/DC condition coverage report.
- `traceability/matrix.csv`: Links to REQ-ERR-001 and VC-ERR-001.
- `screenshots/gdb_branch_trace.png`: GDB screenshot showing breakpoint hits on error handlers.

### 5. Tool Qualification Evidence

- Tool Qualification ID: COV-ASM-002
- Qualification Report: `TQ_COV-ASM-002.pdf` (attached)
- Tool Version: CustomCoverage v1.2
- Qualification Method: DO-178C Tool Qualification Level 1

### 6. Conclusion

All error paths in `calculate_altitude` have been explicitly tested and verified. MC/DC coverage is complete. No untested branches remain. This satisfies DO-178C Section 6.4.3.2 (Error Path Coverage) and Section 6.4.4.2 (Structural Coverage Analysis). Evidence is traceable, auditable, and generated by qualified tools.

---

## Real-World Case Study: Medical Device Firmware (IEC 62304 Class C) — Expanded

**System**: Infusion pump controller using assembly for real-time motor timing loop.

**Challenge**: Ensure timing loop never misses deadline due to register corruption, unhandled interrupt state, or cache effects.

**Solution Implemented**:

1. Created `TEST-TIMER-001` to simulate interrupt latency while executing timing loop.
2. Used hardware performance counters (Intel PCM) to measure cache misses and branch mispredictions.
3. Added `# check: REQ-TMR-001` and `# check: VC-TMR-001` to every instruction in the tight loop.
4. Generated a **cycle-count histogram** proving worst-case execution time (WCET) was within bound.
5. Performed **mutation testing** on all branch conditions in the loop.
6. Documented:
   - WCET = 147 cycles (bound = 150)
   - All register saves/restores verified
   - Interrupt flag state preserved
   - Cache misses < 5 per iteration
   - All 4 mutants killed by test suite

**Certification Evidence Package**:

- `InfusionPump_WCET_Report.pdf`
- `InfusionPump_MutationTest_Report.md`
- `InfusionPump_RegisterIntegrity_Logs.zip`
- `InfusionPump_Traceability_Matrix.csv`
- `Tool_Qualification_PCM_Measurement.pdf`

**Outcome**: IEC 62304 certification granted. Auditors specifically cited the **“comprehensive, traceable, evidence-driven test suite with quantified timing and mutation coverage”** as exemplary.

---

## Tiered Exercises: Building Verifiable Assembly Test Suites — Expanded

### Exercise 1: Basic — Unit Test Implementation with Coverage

**Goal**: Write a complete unit test for a simple assembly function and generate coverage evidence.

**Task**:

- Implement `add_two_numbers` (inputs: `rdi`, `rsi`; output: `rax`; error in `rdx` if overflow)
- Add `# check` tags for input validation, calculation, output assignment, overflow check
- Write a test harness that calls it and verifies `rax == rdi + rsi` or `rdx != 0` on overflow
- Generate a `.log` file indicating pass/fail
- Use `objdump` to generate a coverage report showing which instructions executed

**Deliverable**:

- `add_two_numbers.asm`
- `test_add_two_numbers.asm`
- `test_add_two_numbers.log`
- `coverage_add_two_numbers.txt` (from objdump)

### Exercise 2: Intermediate — Error Path + MC/DC Coverage

**Goal**: Achieve 100% error path coverage and MC/DC.

**Task**:

- Modify `add_two_numbers` to reject inputs > 1000 (return error code in `rdx`)
- Identify all branches leading to error return
- Write one test case per error condition
- For any compound condition (e.g., `if (a > 1000 || b > 1000)`), write tests to prove MC/DC
- Generate a traceability matrix (CSV) linking each test to its requirement and verification tag
- Generate an MC/DC report (CSV)

**Deliverable**:

- Updated `add_two_numbers.asm`
- Five test files (`test_err_001.asm` to `test_err_005.asm`)
- `traceability_matrix.csv`
- `mcdc_report.csv`

### Exercise 3: Advanced — Full Certification Evidence Package with Mutation Testing

**Goal**: Produce a certification-ready evidence bundle including mutation testing.

**Task**:

- Implement a complete assembly module with 3 functions and 5 error paths
- Write unit tests and error path tests for all
- Generate:
  - One Markdown verification report per test (like above)
  - One CSV traceability matrix
  - One coverage report (from objdump + script)
  - One MC/DC report
  - One mutation test report (Python script that injects 5 faults and verifies test suite fails)
  - Tool qualification statement (even if hypothetical: “Custom Python script v1.0, qualified per DO-178C Appendix A”)
- Submit as a ZIP: `cert_evidence_v1.zip`

**Deliverable**:

- All source code
- All test files
- All generated reports
- `README.md` explaining how evidence meets DO-178C Level A / IEC 62304 Class C

---

## Verification Pitfalls to Avoid — Expanded Table

| Pitfall                       | Consequence                         | Mitigation                                  | Certification Reference      |
| ----------------------------- | ----------------------------------- | ------------------------------------------- | ---------------------------- |
| Testing only “happy path”     | Certification rejection             | Enforce 100% error path rule                | DO-178C 6.4.3.2              |
| No `# check` tags             | Evidence untraceable                | Mandate tags on every test block            | DO-178C 5.2                  |
| Using floating-point in tests | Tool qualification complexity       | Restrict tests to integer-only              | DO-178C 12.2                 |
| Assuming compiler behavior    | Undefined results                   | Never rely on compiler optimizations        | DO-178C 6.3.2                |
| Manual test logging           | Inconsistent, unrepeatable          | Automate logging via exit codes and scripts | DO-178C 6.4.2.1              |
| Ignoring processor variants   | Test passes on dev, fails on target | Test on all target hardware variants        | DO-178C 6.3.3                |
| No mutation testing           | Test suite may be insufficient      | Inject faults and verify detection          | DO-178C 6.4.4.3 (Robustness) |
| Unqualified test tools        | All evidence invalid                | Qualify all test tools                      | DO-178C 12.2                 |

---

## Connection to Next Tutorial: Tool Qualification and Automation

In **Tutorial #21**, we will build automated tools to:

- Extract `# check` tags from assembly source → auto-generate traceability matrix
- Auto-generate test stubs from branch analysis (via `objdump`)
- Validate traceability matrices against requirements database
- Generate certification-ready PDF reports with embedded evidence
- Automate mutation testing for all branch instructions
- Integrate with CI/CD pipeline for continuous verification

**Why?** Because manual verification is not scalable. Certification requires **repeatable, tool-assisted evidence**. You cannot manually audit 50,000 lines of assembly for 100% error coverage. But a qualified script can.

> **Final Principle**: _Your test suite is not a collection of programs. It is your certification evidence repository._  
> Treat it with the same rigor as your design documents.
