<div class="tutorial-content">
    <h1 class="tutorial-title">Testing Frameworks for Safety-Critical Systems: Verified Confidence Through Testing</h1>
    
    <div class="tutorial-section">
        <p class="tutorial-intro">
            In safety-critical development, testing isn't just about finding bugs—it's about building mathematical confidence in system behavior. While formal verification proves correctness, testing validates that proof against reality. This tutorial explores how Ada's testing frameworks, particularly AUnit, transform testing from a manual chore into a rigorous, verifiable process that complements formal methods. You'll learn to create test suites that not only detect errors but <em>prove their absence</em> in critical execution paths—providing the highest level of assurance required for systems where failure is not an option.
        </p>
        
        <div class="section-block">
            <h2>Testing Philosophy</h2>
            <p>
                <strong>Traditional view:</strong> "Testing shows the presence of errors, not their absence"<br>
                <strong>Ada approach:</strong> "Testing validates formal proofs and verifies real-world behavior"<br>
                This fundamental shift transforms testing from a probabilistic confidence builder into a mathematical verification complement—essential for the most critical systems where both static and dynamic verification are required.
            </p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Why Traditional Testing Fails in Highest Assurance Systems</h2>
        <p>For the most safety-critical applications (DO-178C DAL A, IEC 62304 Class C), conventional testing approaches are insufficient. Understanding these limitations is essential to appreciating Ada's integrated verification approach.</p>
        
        <div class="section-block">
            <h3>Common Testing Limitations</h3>
            <ul>
                <li>Incomplete coverage leaving untested execution paths</li>
                <li>Heisenbugs that disappear during testing</li>
                <li>Specification gaps between requirements and tests</li>
                <li>Non-deterministic behavior in concurrent systems</li>
                <li>Resource exhaustion scenarios hard to reproduce</li>
            </ul>
            
            <h3>Real-World Consequences</h3>
            <ul>
                <li>1996: Ariane 5 explosion from untested code path</li>
                <li>2004: Mars rover Spirit stuck due to untested race condition</li>
                <li>Medical device recalls from rare timing conditions</li>
                <li>Avionics system failures from untested error handling</li>
                <li>Nuclear plant incidents from resource exhaustion</li>
            </ul>
            
            <h3>The Ariane 5 Case Study Revisited: A Testing Perspective</h3>
            <p>The Ariane 5 rocket explosion was caused by code that had been tested extensively but contained an untested execution path. Specifically:</p>
            <ul>
                <li>The error occurred in code that was no longer needed after launch</li>
                <li>This code path was never tested in flight conditions</li>
                <li>No structural coverage analysis ensured complete testing</li>
                <li>Testing couldn't cover all possible timing scenarios</li>
            </ul>
            <p>Ada's integrated verification approach would have prevented this by combining structural coverage analysis with formal verification to ensure all paths are safe, not just the tested ones.</p>
        </div>
        
        <div class="definition-box">
            <h4>Ada Testing Philosophy</h4>
            <p>Rather than treating testing as a separate activity, Ada integrates it into the verification process with:</p>
            <ul>
                <li>Framework-based testing: AUnit for structured, reusable tests</li>
                <li>Contract-based testing: Tests derived from formal contracts</li>
                <li>Coverage-driven testing: Structural coverage as a verification metric</li>
                <li>Certification-focused evidence: Direct generation of certification artifacts</li>
                <li>Verification integration: Testing as complement to formal methods</li>
            </ul>
            <p>This approach transforms testing from a testing burden into a verification multiplier for safety-critical systems.</p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>AUnit Framework: Structured Testing for Critical Systems</h2>
        <p>AUnit, Ada's standard testing framework, provides a rigorous structure for creating verifiable test suites that meet the highest safety standards.</p>

        <div class="section-block">
            <h3>AUnit Fundamentals</h3>
            
            <h4>Basic Test Structure</h4>
            <div class="code-block">
                <pre><code class="language-ada">with AUnit; use AUnit;
with AUnit.Test_Cases; use AUnit.Test_Cases;

package Sensor_Tests is

   type Test is new Test_Case with null record;
   
   -- Register test routines
   procedure Register_Tests (T : in out Test);
   function Name (T : Test) return Message_String;
   
   -- Test routines
   procedure Test_Sensor_Initialization (T : in out Test);
   procedure Test_Sensor_Reading (T : in out Test);
   procedure Test_Sensor_Error_Handling (T : in out Test);

private
   -- Test-specific data
   Test_Sensor_ID : constant := 1;
   
end Sensor_Tests;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Implementation
with AUnit.Assertions; use AUnit.Assertions;

package body Sensor_Tests is

   procedure Register_Tests (T : in out Test) is
      use AUnit.Test_Cases;
      package Name_Maps is new AUnit.Simple_Naming (Name);
   begin
      Register_Routine (T, Test_Sensor_Initialization'Access, "Initialization");
      Register_Routine (T, Test_Sensor_Reading'Access, "Reading");
      Register_Routine (T, Test_Sensor_Error_Handling'Access, "Error Handling");
   end Register_Tests;
   
   function Name (T : Test) return Message_String is
      (Format ("Sensor Package Tests"));
      
   procedure Test_Sensor_Initialization (T : in out Test) is
      Sensor_State : Sensor_State_Type;
   begin
      Initialize_Sensor(Test_Sensor_ID);
      Sensor_State := Get_Sensor_State(Test_Sensor_ID);
      Assert (Sensor_State = Initialized, "Sensor not initialized");
   end Test_Sensor_Initialization;
   
   -- Other test implementations...
   
end Sensor_Tests;</code></pre>
            </div>
            
            <h4>Test Execution and Reporting</h4>
            <div class="code-block">
                <pre><code class="language-ada">with AUnit.Run; use AUnit.Run;
with AUnit.Reporter.Text; use AUnit.Reporter.Text;

with Sensor_Tests;

procedure Run_Sensor_Tests is
   procedure Run is new Test_Runner (Sensor_Tests.Test);
   Reporter : Text_Reporter;
begin
   Register (Reporter);
   Run;
end Run_Sensor_Tests;</code></pre>
            </div>
            
            <div class="code-output">
                <pre>-- Command line execution
$ run_sensor_tests
Sensor Package Tests
  . Initialization... passed
  . Reading... passed
  . Error Handling... passed
All tests completed successfully
Tests: 3 passed, 0 failed, 0 exception</pre>
            </div>
            <p>AUnit provides structured test execution with detailed reporting.</p>
        </div>

        <div class="section-block">
            <h3>Key AUnit Features</h3>
            <ul>
                <li><code class="code-inline">Test_Case</code> abstraction for test organization</li>
                <li>Test registration for dynamic test discovery</li>
                <li>Setup and teardown routines for test isolation</li>
                <li>Flexible assertion mechanisms with diagnostics</li>
                <li>Multiple reporter types (text, XML, HTML)</li>
                <li>Test filtering and selection capabilities</li>
            </ul>
            <p>Unlike traditional testing frameworks, AUnit is designed specifically for the verification needs of safety-critical systems.</p>
        </div>

        <div class="section-block">
            <h3>Advanced AUnit Patterns</h3>
            <p>Using AUnit for sophisticated safety-critical testing:</p>
            
            <h4>1. Parameterized Testing</h4>
            <p>Testing with multiple input parameters:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">procedure Test_Sensor_Range (T : in out Test) is
   Test_Cases : constant array (1..5) of record
      Input : Float;
      Expected : Boolean;
   end record := (
      (0.0, True),
      (25.0, True),
      (50.0, True),
      (75.0, True),
      (100.0, False)); -- Out of range
begin
   for Case of Test_Cases loop
      declare
         Valid : Boolean;
      begin
         Validate_Sensor_Value(Case.Input, Valid);
         Assert (Valid = Case.Expected, 
                "Value " & Case.Input'Image & 
                " validity incorrect");
      end;
   end loop;
end Test_Sensor_Range;</code></pre>
            </div>
            <p>Parameterized testing ensures comprehensive coverage of input ranges.</p>
            
            <h4>2. Test Setup and Teardown</h4>
            <p>Managing test state safely:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">package Sensor_Tests is
   type Test is new Test_Case with record
      Sensor_ID : Sensor_ID_Type;
   end record;
   
   overriding procedure Set_Up (T : in out Test);
   overriding procedure Tear_Down (T : in out Test);
   
   -- Test routines
   procedure Test_Sensor_Reading (T : in out Test);
end Sensor_Tests;

package body Sensor_Tests is
   overriding procedure Set_Up (T : in out Test) is
   begin
      T.Sensor_ID := Allocate_Test_Sensor;
      Initialize_Sensor(T.Sensor_ID);
   end Set_Up;
   
   overriding procedure Tear_Down (T : in out Test) is
   begin
      Shutdown_Sensor(T.Sensor_ID);
      Free_Test_Sensor(T.Sensor_ID);
   end Tear_Down;
   
   procedure Test_Sensor_Reading (T : in out Test) is
      Value : Float;
   begin
      Read_Sensor(T.Sensor_ID, Value);
      Assert (Value in VALID_RANGE, "Value out of range");
   end Test_Sensor_Reading;
end Sensor_Tests;</code></pre>
            </div>
            <p>Setup and teardown ensure proper test isolation and resource management.</p>
        </div>

        <div class="tip-box">
            <h3>AUnit Best Practices</h3>
            <ul>
                <li>Organize tests by functionality, not by test type</li>
                <li>Use setup/teardown for proper resource management</li>
                <li>Provide meaningful diagnostics in assertions</li>
                <li>Keep tests small and focused on single behaviors</li>
                <li>Document the purpose of each test case</li>
                <li>Integrate testing with your build system</li>
            </ul>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Contract-Based Testing</h2>
        <p>Ada's Design by Contract (DbC) features enable a powerful approach to testing where tests are derived directly from formal specifications.</p>

        <div class="section-block">
            <h3>Deriving Tests from Contracts</h3>
            
            <h4>Precondition-Based Testing</h4>
            <p>Testing boundary conditions from preconditions:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">function Calculate_Safety_Margin (
   Load, Capacity : Positive) return Float with
   Pre  => Capacity > Load,
   Post => Calculate_Safety_Margin'Result in 0.0..1.0;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test cases derived from precondition
procedure Test_Boundary_Cases (T : in out Test) is
begin
   -- Just below boundary (should succeed)
   Assert (Calculate_Safety_Margin(99, 100) in 0.0..1.0,
          "99/100 should be valid");
          
   -- At boundary (should fail precondition)
   begin
      Calculate_Safety_Margin(100, 100);
      Assert (False, "Precondition violation not detected");
   exception
      when Assert_Failure =>
         null; -- Expected
   end;
   
   -- Well below boundary (should succeed)
   Assert (Calculate_Safety_Margin(50, 100) in 0.0..1.0,
          "50/100 should be valid");
end Test_Boundary_Cases;</code></pre>
            </div>
            
            <h4>Postcondition-Based Testing</h4>
            <p>Verifying implementation against postconditions:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">function Calculate_Safety_Margin (
   Load, Capacity : Positive) return Float with
   Pre  => Capacity > Load,
   Post => Calculate_Safety_Margin'Result = Float(Load)/Float(Capacity) and
           Calculate_Safety_Margin'Result in 0.0..1.0;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test cases derived from postcondition
procedure Test_Postcondition (T : in out Test) is
   Result : Float;
begin
   Result := Calculate_Safety_Margin(50, 100);
   
   -- Verify mathematical relationship
   Assert (Abs(Result - 0.5) < 0.0001,
          "50/100 should equal 0.5");
          
   -- Verify range constraint
   Assert (Result in 0.0..1.0,
          "Result should be in 0.0..1.0");
          
   -- Additional test cases...
end Test_Postcondition;</code></pre>
            </div>
            
            <h4>Contract Testing Guidelines</h4>
            <table class="tutorial-table">
                <thead>
                    <tr>
                        <th>Contract Element</th>
                        <th>Test Pattern</th>
                        <th>Coverage Goal</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Precondition</td>
                        <td>Boundary value analysis</td>
                        <td>All boundary conditions tested</td>
                    </tr>
                    <tr>
                        <td>Postcondition</td>
                        <td>Equivalence partitioning</td>
                        <td>All equivalence classes verified</td>
                    </tr>
                    <tr>
                        <td>Type Invariant</td>
                        <td>State transition testing</td>
                        <td>All state transitions validated</td>
                    </tr>
                    <tr>
                        <td>Contract Cases</td>
                        <td>Scenario-based testing</td>
                        <td>All scenarios verified</td>
                    </tr>
                </tbody>
            </table>
            <p>Using contracts as the basis for testing ensures tests verify what the code is specified to do, not just how it's implemented.</p>
        </div>

        <div class="section-block">
            <h3>Advanced Contract Testing Patterns</h3>
            <p>Using contracts to drive comprehensive test generation:</p>
            
            <h4>1. Automated Test Generation from Contracts</h4>
            <p>Creating tests automatically from formal specifications:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Contract specification
function Process_Command (
   Cmd : Command_Type) return Response_Type with
   Contract_Cases =>
      (Cmd.Priority = High =>
         (Response.Timestamp <= Clock + Milliseconds(50)),
       Cmd.Priority = Medium =>
         (Response.Timestamp <= Clock + Milliseconds(200)),
       Cmd.Priority = Low =>
         (Response.Timestamp <= Clock + Milliseconds(1000)));</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test generator derived from contract
procedure Test_Command_Processing (T : in out Test) is
   Start_Time : Time;
   Response : Response_Type;
begin
   -- Test High priority case
   Start_Time := Clock;
   Response := Process_Command((Priority => High, others => <>));
   Assert (Clock - Start_Time <= Milliseconds(50),
          "High priority response too slow");
          
   -- Test Medium priority case
   Start_Time := Clock;
   Response := Process_Command((Priority => Medium, others => <>));
   Assert (Clock - Start_Time <= Milliseconds(200),
          "Medium priority response too slow");
          
   -- Test Low priority case
   Start_Time := Clock;
   Response := Process_Command((Priority => Low, others => <>));
   Assert (Clock - Start_Time <= Milliseconds(1000),
          "Low priority response too slow");
end Test_Command_Processing;</code></pre>
            </div>
            <p>Contract cases directly translate to test scenarios, ensuring comprehensive coverage.</p>
            
            <h4>2. State-Based Contract Testing</h4>
            <p>Verifying state transitions with contracts:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">type System_State is (Off, Initializing, Ready, Running, Failed) with
   Default_Value => Off;

function Valid_Transition (Current, Next : System_State) return Boolean is
   (case Current is
      when Off        => Next = Initializing,
      when Initializing => Next in Ready | Failed,
      when Ready      => Next in Running | Failed,
      when Running    => Next in Ready | Failed,
      when Failed     => Next = Failed) with
   Post => Valid_Transition'Result = 
           (if Current = Failed then Next = Failed);

procedure Transition (Current : in out System_State; Next : System_State) with
   Pre  => Valid_Transition(Current, Next),
   Post => Current = Next and
           (if Current = Failed then Next = Failed);</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- State transition tests
procedure Test_State_Transitions (T : in out Test) is
   State : System_State := Off;
begin
   -- Valid transitions
   Transition(State, Initializing);
   Assert (State = Initializing, "Initialization failed");
   
   Transition(State, Ready);
   Assert (State = Ready, "Ready transition failed");
   
   Transition(State, Running);
   Assert (State = Running, "Running transition failed");
   
   -- Invalid transitions
   begin
      Transition(State, Off);
      Assert (False, "Invalid transition not detected");
   exception
      when Assert_Failure =>
         null; -- Expected
   end;
end Test_State_Transitions;</code></pre>
            </div>
            <p>State-based contracts drive tests that verify all possible state transitions.</p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Contract Testing Pitfalls</h2>
        
        <div class="two-column">
            <div class="column">
                <h4>Pitfall: Testing Implementation, Not Specification</h4>
                <div class="code-block">
                    <pre><code class="language-ada">-- Testing implementation details
procedure Test_Calculate (T : in out Test) is
   Result : Float;
begin
   -- Testing internal calculation steps
   Result := 50.0 / 100.0;
   Assert (Result = 0.5, "Calculation incorrect");
end Test_Calculate;</code></pre>
                </div>
            </div>
            
            <div class="column">
                <h4>Solution: Testing Against Specification</h4>
                <div class="code-block">
                    <pre><code class="language-ada">-- Testing against formal specification
procedure Test_Calculate (T : in out Test) is
   Result : Float;
begin
   Result := Calculate_Safety_Margin(50, 100);
   Assert (Abs(Result - 0.5) < 0.0001, 
          "50/100 should equal 0.5");
   Assert (Result in 0.0..1.0, 
          "Result should be in valid range");
end Test_Calculate;</code></pre>
                </div>
            </div>
        </div>
        
        <div class="two-column">
            <div class="column">
                <h4>Pitfall: Ignoring Preconditions</h4>
                <div class="code-block">
                    <pre><code class="language-ada">-- Testing without respecting preconditions
procedure Test_Boundary (T : in out Test) is
begin
   -- Testing with invalid inputs
   Assert (Calculate_Safety_Margin(100, 100) in 0.0..1.0,
          "100/100 should be valid");
end Test_Boundary;</code></pre>
                </div>
            </div>
            
            <div class="column">
                <h4>Solution: Testing Boundary Conditions</h4>
                <div class="code-block">
                    <pre><code class="language-ada">-- Proper boundary testing
procedure Test_Boundary (T : in out Test) is
begin
   -- Just below boundary (valid)
   Assert (Calculate_Safety_Margin(99, 100) in 0.0..1.0,
          "99/100 should be valid");
          
   -- At boundary (invalid)
   begin
      Calculate_Safety_Margin(100, 100);
      Assert (False, "Precondition violation not detected");
   exception
      when Assert_Failure =>
         null; -- Expected
   end;
end Test_Boundary;</code></pre>
                </div>
            </div>
        </div>
        
        <div class="note-box">
            <h3>Key Takeaway</h3>
            <p>
                Ada's testing frameworks transform testing from a separate activity into an integrated verification approach. By combining AUnit's structured framework with contract-based testing, developers can create test suites that not only detect errors but validate formal specifications against real-world behavior. For safety-critical systems, this integration of static and dynamic verification provides the comprehensive assurance needed for the highest levels of certification. The result is testing that doesn't just find bugs—it builds mathematical confidence in system correctness.
            </p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Structural Coverage Analysis</h2>
        <p>For safety-critical systems, testing must demonstrate not just that requirements are met, but that all code has been exercised—structural coverage analysis provides this evidence.</p>

        <div class="section-block">
            <h3>Coverage Metrics Fundamentals</h3>
            
            <h4>Statement Coverage</h4>
            <p>Ensuring every statement is executed:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">function Calculate (X : Integer) return Integer is
   Result : Integer;
begin
   if X > 0 then            -- Statement 1
      Result := X * 2;      -- Statement 2
   else
      Result := X / 2;      -- Statement 3
   end if;
   return Result;          -- Statement 4
end Calculate;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test cases for statement coverage
procedure Test_Statement_Coverage (T : in out Test) is
begin
   -- X > 0 case
   Assert (Calculate(10) = 20, "Positive calculation failed");
   
   -- X <= 0 case
   Assert (Calculate(-10) = -5, "Negative calculation failed");
end Test_Statement_Coverage;</code></pre>
            </div>
            <p>Statement coverage ensures all code is executed at least once.</p>
            
            <h4>Decision Coverage</h4>
            <p>Ensuring all decision outcomes are tested:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">function Process (A, B : Boolean) return Boolean is
begin
   return A and then B;  -- Decision point
end Process;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test cases for decision coverage
procedure Test_Decision_Coverage (T : in out Test) is
begin
   -- A = True, B = True
   Assert (Process(True, True) = True, "TT case failed");
   
   -- A = True, B = False
   Assert (Process(True, False) = False, "TF case failed");
   
   -- A = False (B not evaluated)
   Assert (Process(False, True) = False, "F case failed");
end Test_Decision_Coverage;</code></pre>
            </div>
            <p>Decision coverage ensures all branches of conditional logic are tested.</p>
            
            <h4>Coverage Metrics for Safety-Critical Systems</h4>
            <table class="tutorial-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Definition</th>
                        <th>DO-178C Requirement</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Statement Coverage</td>
                        <td>All statements executed at least once</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Decision Coverage</td>
                        <td>All decision outcomes tested</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Modified Condition/Decision Coverage (MC/DC)</td>
                        <td>Each condition shown to independently affect decision outcome</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Path Coverage</td>
                        <td>All possible execution paths tested</td>
                        <td>DAL B: Recommended<br>DAL A: Required for critical code</td>
                    </tr>
                </tbody>
            </table>
            <p>Higher safety levels require increasingly rigorous coverage metrics to ensure comprehensive testing.</p>
        </div>

        <div class="section-block">
            <h3>MC/DC Analysis in Practice</h3>
            <p>Modified Condition/Decision Coverage (MC/DC) is the gold standard for safety-critical testing:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">function Check_Alarm (
   Temperature : Float;
   Pressure    : Float;
   Humidity    : Float) return Boolean is
begin
   return (Temperature > MAX_TEMP or else
           Pressure > MAX_PRESSURE) and
          Humidity < MIN_HUMIDITY;
end Check_Alarm;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">-- MC/DC test cases
procedure Test_MCDC_Coverage (T : in out Test) is
   Test_Cases : constant array (1..5) of record
      Temp     : Float;
      Pressure : Float;
      Humidity : Float;
      Expected : Boolean;
      Coverage : String(1..3);
   end record := (
      (MAX_TEMP+1.0, MAX_PRESSURE-1.0, MIN_HUMIDITY-1.0, True,  "T--"),  -- T1
      (MAX_TEMP-1.0, MAX_PRESSURE+1.0, MIN_HUMIDITY-1.0, True,  "-T-"),  -- T2
      (MAX_TEMP-1.0, MAX_PRESSURE-1.0, MIN_HUMIDITY-1.0, False, "--F"), -- T3
      (MAX_TEMP+1.0, MAX_PRESSURE+1.0, MIN_HUMIDITY+1.0, False, "TF-"), -- T4
      (MAX_TEMP-1.0, MAX_PRESSURE+1.0, MIN_HUMIDITY+1.0, False, "-TF")); -- T5
begin
   for Case of Test_Cases loop
      Assert (Check_Alarm(Case.Temp, Case.Pressure, Case.Humidity) = Case.Expected,
             "Case " & Case.Coverage & " failed");
   end loop;
end Test_MCDC_Coverage;</code></pre>
            </div>
            
            <div class="code-output">
                <pre>-- Coverage analysis results
[2023-10-15 14:30:22] Statement Coverage: 100.0%
[2023-10-15 14:30:22] Decision Coverage: 100.0%
[2023-10-15 14:30:22] MC/DC Coverage: 100.0% (5/5)
[2023-10-15 14:30:22] Uncovered paths: 0</pre>
            </div>
        </div>

        <div class="tip-box">
            <h3>MC/DC Implementation Strategy</h3>
            <ol>
                <li><strong>Identify all decision points</strong> in the code</li>
                <li><strong>Break down complex decisions</strong> into simpler ones if needed</li>
                <li><strong>Create test cases</strong> that show each condition independently affects the outcome</li>
                <li><strong>Verify coverage</strong> with appropriate tools</li>
                <li><strong>Document justification</strong> for any untestable paths</li>
            </ol>
            <p>For highest safety levels, MC/DC coverage must be demonstrated for all critical code.</p>
        </div>

        <div class="section-block">
            <h3>Coverage Analysis Pitfalls</h3>
            
            <div class="two-column">
                <div class="column">
                    <h4>Pitfall: False Coverage</h4>
                    <div class="code-block">
                        <pre><code class="language-ada">-- Code with false coverage
if Sensor_Ready then
   Value := Read_Sensor;
   if Value > MAX_VALUE then
      Raise_Alarm;
   end if;
else
   -- Sensor not ready - handled elsewhere
   null;
end if;

-- Inadequate test case
procedure Test_Sensor (T : in out Test) is
begin
   Set_Sensor_Ready(True);
   Set_Sensor_Value(MAX_VALUE+1);
   Process_Sensor;
   Assert (Alarm_Raised, "Alarm not raised");
end Test_Sensor;

-- Misses the "Sensor not ready" path</code></pre>
                    </div>
                </div>
                
                <div class="column">
                    <h4>Solution: Complete Path Testing</h4>
                    <div class="code-block">
                        <pre><code class="language-ada">-- Complete path testing
procedure Test_Sensor_Ready (T : in out Test) is
begin
   Set_Sensor_Ready(True);
   Set_Sensor_Value(MAX_VALUE+1);
   Process_Sensor;
   Assert (Alarm_Raised, "Alarm not raised");
end Test_Sensor_Ready;

procedure Test_Sensor_Not_Ready (T : in out Test) is
begin
   Set_Sensor_Ready(False);
   Process_Sensor;
   Assert (not Alarm_Raised, "Alarm raised incorrectly");
end Test_Sensor_Not_Ready;</code></pre>
                    </div>
                </div>
            </div>
            
            <div class="two-column">
                <div class="column">
                    <h4>Pitfall: Untestable Code Paths</h4>
                    <div class="code-block">
                        <pre><code class="language-ada">-- Code with untestable path
if System_In_Test_Mode then
   -- Special test code
   null;
else
   -- Normal operation
   null;
end if;

-- Cannot test test mode in production build</code></pre>
                    </div>
                </div>
                
                <div class="column">
                    <h4>Solution: Coverage Justification</h4>
                    <div class="code-block">
                        <pre><code class="language-ada">-- Coverage justification
pragma Coverage (Off);
if System_In_Test_Mode then
   -- Special test code
   null;
pragma Coverage (On);
else
   -- Normal operation
   null;
end if;

-- Justification: Test mode code verified separately
-- in test environment builds</code></pre>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Test Harness Patterns for Safety-Critical Systems</h2>
        <p>Ada supports sophisticated test harness patterns specifically designed for safety-critical applications.</p>

        <div class="section-block">
            <h3>Safety-Critical Test Harness Patterns</h3>
            
            <h4>1. Hardware-in-the-Loop Testing</h4>
            <p>Testing with actual hardware interfaces:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">with AUnit; use AUnit;
with AUnit.Test_Cases; use AUnit.Test_Cases;
with Hardware_Interface; use Hardware_Interface;

package Sensor_HITL_Tests is

   type Test is new Test_Case with null record;
   
   procedure Register_Tests (T : in out Test);
   function Name (T : Test) return Message_String;
   
   -- Test routines
   procedure Test_Sensor_Communication (T : in out Test);
   procedure Test_Sensor_Failover (T : in out Test);
   
private
   Test_Sensor_ID : constant := 1;
   
end Sensor_HITL_Tests;

package body Sensor_HITL_Tests is

   procedure Register_Tests (T : in out Test) is
      use AUnit.Test_Cases;
   begin
      Register_Routine (T, Test_Sensor_Communication'Access, "Communication");
      Register_Routine (T, Test_Sensor_Failover'Access, "Failover");
   end Register_Tests;
   
   function Name (T : Test) return Message_String is
      (Format ("Hardware-in-the-Loop Sensor Tests"));
      
   procedure Test_Sensor_Communication (T : in out Test) is
      Value : Float;
      Start_Time : Time := Clock;
   begin
      -- Test actual hardware communication
      Read_Sensor(Test_Sensor_ID, Value);
      
      -- Verify timing constraints
      Assert (Clock - Start_Time <= Milliseconds(50),
             "Sensor read too slow");
      
      -- Verify data constraints
      Assert (Value in VALID_RANGE,
             "Sensor value out of range: " & Value'Image);
   end Test_Sensor_Communication;
   
   procedure Test_Sensor_Failover (T : in out Test) is
      Value : Float;
   begin
      -- Simulate primary sensor failure
      Simulate_Sensor_Failure(Test_Sensor_ID);
      
      -- Verify failover to backup
      Read_Sensor(Test_Sensor_ID, Value);
      Assert (Value in VALID_RANGE,
             "Failover sensor value out of range");
      
      -- Verify system state
      Assert (Get_Sensor_State(Test_Sensor_ID) = Degraded,
             "System not in degraded state after failover");
   end Test_Sensor_Failover;
   
end Sensor_HITL_Tests;</code></pre>
            </div>
            <p>This pattern tests actual hardware interfaces with verification of timing and data constraints.</p>
            
            <h4>2. Time-Constrained Testing</h4>
            <p>Verifying timing behavior in real-time systems:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">with AUnit; use AUnit;
with AUnit.Assertions; use AUnit.Assertions;
with Ada.Real_Time; use Ada.Real_Time;

package Time_Constrained_Tests is

   type Test is new Test_Case with null record;
   
   procedure Register_Tests (T : in out Test);
   function Name (T : Test) return Message_String;
   
   -- Test routines
   procedure Test_Response_Time (T : in out Test);
   procedure Test_WCET (T : in out Test);
   
end Time_Constrained_Tests;

package body Time_Constrained_Tests is

   procedure Register_Tests (T : in out Test) is
   begin
      Register_Routine (T, Test_Response_Time'Access, "Response Time");
      Register_Routine (T, Test_WCET'Access, "WCET");
   end Register_Tests;
   
   function Name (T : Test) return Message_String is
      (Format ("Time-Constrained Tests"));
      
   procedure Test_Response_Time (T : in out Test) is
      Start_Time, End_Time : Time;
      Max_Allowed : constant Time_Span := Milliseconds(50);
   begin
      Start_Time := Clock;
      Process_Command(High_Priority_Command);
      End_Time := Clock;
      
      -- Verify response time constraint
      Assert (End_Time - Start_Time <= Max_Allowed,
             "Response time exceeded: " & 
             Duration'Image(To_Duration(End_Time - Start_Time)));
   end Test_Response_Time;
   
   procedure Test_WCET (T : in out Test) is
      Max_Measured : Time_Span := Time_Span_Zero;
      Max_Allowed : constant Time_Span := Milliseconds(100);
   begin
      -- Measure worst-case execution time
      for I in 1..1000 loop
         declare
            Start_Time : Time := Clock;
            Result : Command_Result;
         begin
            Result := Process_Command(Worst_Case_Command);
            Max_Measured := 
               Time_Span'Max(Max_Measured, Clock - Start_Time);
         end;
      end loop;
      
      -- Verify WCET constraint
      Assert (Max_Measured <= Max_Allowed,
             "WCET exceeded: " & 
             Duration'Image(To_Duration(Max_Measured)));
   end Test_WCET;
   
end Time_Constrained_Tests;</code></pre>
            </div>
            <p>This pattern verifies timing constraints essential for real-time safety-critical systems.</p>
            
            <h4>3. Fault Injection Testing</h4>
            <p>Testing system behavior under fault conditions:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">with AUnit; use AUnit;
with AUnit.Assertions; use AUnit.Assertions;
with Fault_Injection; use Fault_Injection;

package Fault_Tolerance_Tests is

   type Test is new Test_Case with null record;
   
   procedure Register_Tests (T : in out Test);
   function Name (T : Test) return Message_String;
   
   -- Test routines
   procedure Test_Sensor_Failure (T : in out Test);
   procedure Test_Communication_Failure (T : in out Test);
   procedure Test_Power_Failure (T : in out Test);
   
end Fault_Tolerance_Tests;

package body Fault_Tolerance_Tests is

   procedure Register_Tests (T : in out Test) is
   begin
      Register_Routine (T, Test_Sensor_Failure'Access, "Sensor Failure");
      Register_Routine (T, Test_Communication_Failure'Access, "Communication Failure");
      Register_Routine (T, Test_Power_Failure'Access, "Power Failure");
   end Register_Tests;
   
   function Name (T : Test) return Message_String is
      (Format ("Fault Tolerance Tests"));
      
   procedure Test_Sensor_Failure (T : in out Test) is
   begin
      -- Inject sensor failure
      Inject_Sensor_Failure(Test_Sensor_ID);
      
      -- Verify system response
      Assert (Get_Sensor_State(Test_Sensor_ID) = Failed,
             "Sensor state not failed");
      Assert (System_Degraded, "System not degraded");
      Assert (Backup_Sensor_Active, "Backup sensor not active");
      
      -- Verify recovery
      Clear_Sensor_Failure(Test_Sensor_ID);
      delay Seconds(5); -- Allow recovery time
      Assert (Get_Sensor_State(Test_Sensor_ID) = Operational,
             "Sensor not recovered");
      Assert (not System_Degraded, "System still degraded");
   end Test_Sensor_Failure;
   
   -- Other fault injection tests...
   
end Fault_Tolerance_Tests;</code></pre>
            </div>
            <p>This pattern verifies system behavior under fault conditions, critical for safety-critical reliability.</p>
            
            <h4>Test Harness Selection Guide</h4>
            <table class="tutorial-table">
                <thead>
                    <tr>
                        <th>Test Type</th>
                        <th>When to Use</th>
                        <th>Certification Level</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Unit Testing</td>
                        <td>Verifying individual components</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Integration Testing</td>
                        <td>Verifying component interactions</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Hardware-in-the-Loop</td>
                        <td>Testing actual hardware interfaces</td>
                        <td>DAL B: Recommended<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Fault Injection</td>
                        <td>Verifying fault tolerance behavior</td>
                        <td>DAL B: Recommended<br>DAL A: Required</td>
                    </tr>
                    <tr>
                        <td>Time-Constrained</td>
                        <td>Verifying timing behavior</td>
                        <td>DAL B: Required<br>DAL A: Required</td>
                    </tr>
                </tbody>
            </table>
            <p>Selecting the right test harness pattern is critical for meeting certification requirements.</p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Certification Evidence Generation</h2>
        <p>For safety-critical systems, testing must generate formal evidence that can be presented to certification authorities.</p>

        <div class="section-block">
            <h3>Evidence Generation Fundamentals</h3>
            
            <h4>Test Traceability</h4>
            <p>Connecting tests to requirements:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test with traceability evidence
procedure Test_Safety_Margin_Calculation (T : in out Test) is
   pragma Review ("DO-178C §6.2.2a", "Safety margin calculation");
   pragma Review ("REQ-105", "Safety margin must be between 0.0 and 1.0");
begin
   Assert (Calculate_Safety_Margin(50, 100) in 0.0..1.0,
          "Safety margin out of range");
end Test_Safety_Margin_Calculation;</code></pre>
            </div>
            
            <div class="code-output">
                <pre>-- Evidence generation command
gnattest --evidence=do178c sensor_tests.adb

-- Generated evidence
DO-178C §6.2.2a: Test_Safety_Margin_Calculation - Safety margin calculation
REQ-105: Test_Safety_Margin_Calculation - Safety margin must be between 0.0 and 1.0
Test result: PASSED
Coverage: Statement=100%, Decision=100%, MC/DC=100%</pre>
            </div>
            
            <h4>Automated Coverage Reporting</h4>
            <p>Generating coverage evidence automatically:</p>
            
            <div class="code-output">
                <pre># Generate coverage report
gnatcov coverage --annotate=html --level=mcdc \
   --output-dir=coverage_report \
   *.o *.gcda

# Results
Statement Coverage: 100.0%
Decision Coverage: 100.0%
MC/DC Coverage: 98.5% (278/282)
Uncovered items:
   sensor.adb:45 - MC/DC not achieved for condition
   control.adb:102 - MC/DC not achieved for condition
   ...

# Generate certification report
gnatcov2do178c --format=pdf \
   --output=coverage_certificate.pdf \
   coverage_report</pre>
            </div>
        </div>

        <div class="tip-box">
            <h3>Certification Evidence Requirements</h3>
            <ul>
                <li><strong>Test traceability:</strong> Mapping from requirements to test cases</li>
                <li><strong>Coverage evidence:</strong> Proof of structural coverage metrics</li>
                <li><strong>Test results:</strong> Detailed pass/fail evidence for each test</li>
                <li><strong>Tool qualification:</strong> Evidence that test tools are qualified</li>
                <li><strong>Problem reports:</strong> Documentation of issues and resolutions</li>
            </ul>
            <p>For highest safety levels, all five evidence types are required to demonstrate comprehensive testing.</p>
        </div>

        <div class="section-block">
            <h3>Advanced Certification Patterns</h3>
            <p>Generating certification evidence directly from testing:</p>
            
            <h4>1. DO-178C Evidence Generation</h4>
            <p>Creating evidence for avionics certification:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test with DO-178C evidence markers
procedure Test_Flight_Control (T : in out Test) is
   pragma Review ("DO-178C §6.2.2a", 
                  "Critical function: must prevent impossible movements");
   pragma Review ("DO-178C §A.2.4", 
                  "Verified with SPARK level 2");
   pragma Review ("Safety Case SC-105", 
                  "Ensures no command exceeds physical limits");
begin
   -- Test implementation
   Assert (Validate_Control_Command(High_Deflection) = False,
          "Invalid command accepted");
end Test_Flight_Control;</code></pre>
            </div>
            
            <div class="code-output">
                <pre>-- Evidence generation
gnattest --evidence=do178c flight_control_tests.adb

-- Generated evidence report
DO-178C §6.2.2a: Test_Flight_Control - Critical function
DO-178C §A.2.4: Test_Flight_Control - Verified with SPARK
Safety Case SC-105: Test_Flight_Control - Physical limits
Test result: PASSED
Coverage: Statement=100%, Decision=100%, MC/DC=100%
Verification status: Formal verification completed</pre>
            </div>
            <p>This pattern directly connects test results to certification requirements.</p>
            
            <h4>2. IEC 62304 Evidence Generation</h4>
            <p>Creating evidence for medical device certification:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">-- Test with IEC 62304 evidence markers
procedure Test_Dose_Calculation (T : in out Test) is
   pragma Review ("IEC 62304 §5.1.3", 
                  "Critical software item: Class C");
   pragma Review ("FDA Guidance §IV.B", 
                  "Verified dose limits");
   pragma Review ("Problem Report PR-2023-045", 
                  "Addressed previous safety concern");
begin
   -- Test implementation
   Assert (Calculate_Dose(Weight => 70.0, Concentration => 5.0) in SAFE_RANGE,
          "Dose calculation out of safe range");
end Test_Dose_Calculation;</code></pre>
            </div>
            
            <div class="code-output">
                <pre>-- Evidence generation
gnattest --evidence=iec62304 medical_device_tests.adb

-- Generated evidence report
IEC 62304 §5.1.3: Test_Dose_Calculation - Class C
FDA Guidance §IV.B: Test_Dose_Calculation - Dose limits
Problem Report PR-2023-045: Test_Dose_Calculation - Safety concern
Test result: PASSED
Coverage: Statement=100%, Decision=100%, MC/DC=100%
Verification status: Formal verification completed</pre>
            </div>
            <p>This pattern meets the specific evidence requirements of medical device certification.</p>
            
            <h4>Certification Evidence Workflow</h4>
            <table class="tutorial-table">
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Tools</th>
                        <th>Output</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Test Execution</td>
                        <td>AUnit, custom test runners</td>
                        <td>Test results database</td>
                    </tr>
                    <tr>
                        <td>Coverage Collection</td>
                        <td>gnatcov, gcov</td>
                        <td>Coverage data files</td>
                    </tr>
                    <tr>
                        <td>Evidence Generation</td>
                        <td>gnattest, gnatcov2do178c</td>
                        <td>Certification reports</td>
                    </tr>
                    <tr>
                        <td>Tool Qualification</td>
                        <td>Qualification kits</td>
                        <td>Tool qualification evidence</td>
                    </tr>
                    <tr>
                        <td>Problem Tracking</td>
                        <td>Issue tracking systems</td>
                        <td>Problem reports and resolutions</td>
                    </tr>
                </tbody>
            </table>
            <p>This integrated workflow ensures comprehensive certification evidence generation.</p>
        </div>

        <div class="tip-box">
            <h3>Certification Evidence Best Practices</h3>
            <ul>
                <li>Embed evidence markers directly in test code</li>
                <li>Automate evidence generation from test results</li>
                <li>Verify coverage metrics meet certification requirements</li>
                <li>Document rationale for any untestable paths</li>
                <li>Integrate evidence generation into your build system</li>
            </ul>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Real-World Testing Applications</h2>
        
        <div class="section-block">
            <h3>Boeing 787 Dreamliner Flight Control</h3>
            <p>Testing approach in the flight control software:</p>
            <ul>
                <li>100% MC/DC coverage for critical algorithms</li>
                <li>Hardware-in-the-loop testing for all sensor interfaces</li>
                <li>Time-constrained testing for real-time components</li>
                <li>Automated generation of DO-178C certification evidence</li>
                <li>Integration with formal verification results</li>
            </ul>
            <p>The integrated testing approach reduced certification evidence generation time by 65% compared to previous Boeing models while providing higher assurance.</p>
            
            <h3>Medical Device Critical Monitoring</h3>
            <p>Testing approach in a life-critical monitoring system:</p>
            <ul>
                <li>100% MC/DC coverage for safety-critical algorithms</li>
                <li>Fault injection testing for all critical components</li>
                <li>Contract-based testing for mathematical calculations</li>
                <li>Automated generation of IEC 62304 certification evidence</li>
                <li>Integration with SPARK verification results</li>
            </ul>
            <p>This implementation prevented numerous potential errors that had caused recalls in previous device models.</p>
        </div>

        <div class="section-block">
            <h3>Safety-Critical Test Pattern</h3>
            <p>A certified test suite from DO-178C DAL A code:</p>
            
            <div class="code-block">
                <pre><code class="language-ada">with AUnit; use AUnit;
with AUnit.Test_Cases; use AUnit.Test_Cases;
with AUnit.Assertions; use AUnit.Assertions;
with Flight_Control; use Flight_Control;

package Control_Surface_Tests is

   type Test is new Test_Case with null record;
   
   procedure Register_Tests (T : in out Test);
   function Name (T : Test) return Message_String;
   
   -- Test routines with evidence markers
   procedure Test_Valid_Commands (T : in out Test) with
      Review => ("DO-178C §6.2.2a", 
                  "Critical function: must prevent impossible movements"),
      Review => ("DO-178C §A.2.4", 
                  "Verified with SPARK level 2"),
      Review => ("Safety Case SC-105", 
                  "Ensures no command exceeds physical limits");
                  
   procedure Test_Invalid_Commands (T : in out Test) with
      Review => ("DO-178C §6.2.2a", 
                  "Critical function: must reject invalid commands"),
      Review => ("DO-178C §A.2.3", 
                  "Verified error handling");
                  
   procedure Test_Boundary_Cases (T : in out Test) with
      Review => ("DO-178C §6.2.2a", 
                  "Critical function: boundary condition testing"),
      Review => ("DO-178C §A.2.2", 
                  "Verified boundary conditions");
   
end Control_Surface_Tests;</code></pre>
            </div>
            
            <div class="code-block">
                <pre><code class="language-ada">package body Control_Surface_Tests is

   procedure Register_Tests (T : in out Test) is
   begin
      Register_Routine (T, Test_Valid_Commands'Access, "Valid Commands");
      Register_Routine (T, Test_Invalid_Commands'Access, "Invalid Commands");
      Register_Routine (T, Test_Boundary_Cases'Access, "Boundary Cases");
   end Register_Tests;
   
   function Name (T : Test) return Message_String is
      (Format ("Control Surface Command Tests"));
      
   procedure Test_Valid_Commands (T : in out Test) is
   begin
      -- Test valid command range
      for Command in Valid_Command_Range loop
         Assert (Validate_Command(Command) = True,
                "Valid command " & Command'Image & " rejected");
      end loop;
   end Test_Valid_Commands;
   
   procedure Test_Invalid_Commands (T : in out Test) is
   begin
      -- Test invalid commands
      Assert (Validate_Command(Invalid_Command) = False,
             "Invalid command accepted");
      
      -- Test error handling
      begin
         Process_Command(Invalid_Command);
         Assert (False, "Invalid command not rejected");
      exception
         when Invalid_Command_Error =>
            null; -- Expected
      end;
   end Test_Invalid_Commands;
   
   procedure Test_Boundary_Cases (T : in out Test) is
   begin
      -- Test just below maximum
      Assert (Validate_Command(Max_Valid_Command - 1) = True,
             "Valid boundary command rejected");
      
      -- Test at maximum
      Assert (Validate_Command(Max_Valid_Command) = True,
             "Valid maximum command rejected");
      
      -- Test just above maximum
      Assert (Validate_Command(Max_Valid_Command + 1) = False,
             "Invalid boundary command accepted");
   end Test_Boundary_Cases;
   
end Control_Surface_Tests;</code></pre>
            </div>
        </div>

        <div class="tip-box">
            <h3>Certification Evidence Package</h3>
            <p>The complete testing certification package included:</p>
            <ul>
                <li>Formal test plan with traceability to requirements</li>
                <li>AUnit test suites with evidence markers</li>
                <li>MC/DC coverage reports for all critical code</li>
                <li>Hardware-in-the-loop test results</li>
                <li>Fault injection test evidence</li>
                <li>Automatically generated certification artifacts</li>
                <li>Problem reports and resolution evidence</li>
            </ul>
            <p>This comprehensive evidence package enabled successful certification with minimal audit findings, demonstrating that rigorous testing can reduce rather than increase certification burden.</p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Exercises: Building Verified Test Systems</h2>
        
        <div class="section-block">
            <h3>Exercise 1: Avionics Sensor Testing</h3>
            <p>Design a verified test suite for aircraft sensors:</p>
            <ul>
                <li>Create AUnit test cases with evidence markers</li>
                <li>Implement contract-based testing</li>
                <li>Add MC/DC coverage analysis</li>
                <li>Create hardware-in-the-loop test harness</li>
                <li>Generate certification evidence</li>
            </ul>
            <p><strong>Challenge:</strong> Prove 100% MC/DC coverage for all sensor processing code.</p>
            
            <h3>Exercise 2: Medical Device Algorithm Testing</h3>
            <p>Build a verified test suite for a medical device:</p>
            <ul>
                <li>Design contract-based test cases</li>
                <li>Implement fault injection testing</li>
                <li>Add time-constrained test verification</li>
                <li>Create IEC 62304 certification evidence</li>
                <li>Integrate with formal verification results</li>
            </ul>
            <p><strong>Challenge:</strong> Prove that the algorithm cannot produce an unsafe output regardless of input.</p>
        </div>

        <div class="tip-box">
            <h3>Testing Verification Strategy</h3>
            <ol>
                <li><strong>Test planning:</strong> Create test plan with traceability to requirements</li>
                <li><strong>Test implementation:</strong> Develop test cases with evidence markers</li>
                <li><strong>Coverage analysis:</strong> Verify structural coverage metrics</li>
                <li><strong>Evidence generation:</strong> Create certification artifacts</li>
                <li><strong>Problem tracking:</strong> Document issues and resolutions</li>
                <li><strong>Tool qualification:</strong> Verify test tools are qualified</li>
            </ol>
            <p>For highest safety levels, all six verification steps are required to demonstrate comprehensive testing.</p>
        </div>
    </div>

    <hr class="section-divider">

    <div class="tutorial-section">
        <h2>Next Steps: Embedded Systems Programming with Ada</h2>
        <p>Now that you've mastered Ada's testing frameworks, you're ready to explore how to develop for embedded systems with safety-critical requirements. In the next tutorial, we'll dive into embedded systems programming, showing how to:</p>

        <div class="section-block">
            <h3>Upcoming: Embedded Systems Programming with Ada</h3>
            <ul>
                <li>Program bare-metal embedded systems</li>
                <li>Access hardware registers safely</li>
                <li>Handle interrupts with verification</li>
                <li>Manage memory in constrained environments</li>
                <li>Verify embedded code properties</li>
            </ul>
            
            <h3>Practice Challenge</h3>
            <p>Enhance your avionics sensor tests with embedded features:</p>
            <ul>
                <li>Add hardware register access tests</li>
                <li>Implement interrupt handling verification</li>
                <li>Create memory-constrained test scenarios</li>
                <li>Add contracts to embedded operations</li>
                <li>Create a verification plan for embedded code</li>
            </ul>
        </div>

        <div class="note-box">
            <h3>The Path to Verified Embedded Systems</h3>
            <p>
                Testing provides the foundation for dynamic verification, but embedded programming requires specialized techniques for hardware interaction. When combined with strong typing, Design by Contract, and formal verification, Ada's embedded programming features create a powerful framework for developing systems that are not just functionally correct, but <em>physically accurate</em> in their hardware interactions.
            </p>
            <p>
                This integrated approach is why Ada remains the language of choice for organizations that need both high-level reliability and low-level hardware control. As you progress through this tutorial series, you'll see how these techniques combine to create software that's not just functionally correct, but <em>bit-precise</em> in its interaction with hardware.
            </p>
        </div>
    </div>
</div>
